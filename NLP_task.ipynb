{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1:\n",
    "What has the model learned about the correspondence between the item name and the classification? \n",
    "Answer--> 7up and sprite are classified as \"lemon-line\"\n",
    "          Coke and pepsi are classified as \"cola\"\n",
    "\n",
    "Problem:\n",
    "Coke and pepsi could have different flavor/taste, however the model categorized them all together as \"cola\". The model is not well specialized.\n",
    "\n",
    "Solution:\n",
    "Retrain the model with more specific labels in order to seprate the items into a wider range of flavors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy==1.7.3 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from -r requirements.txt (line 1)) (1.7.3)\n",
      "Requirement already satisfied: ipykernel==6.15.3 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from -r requirements.txt (line 2)) (6.15.3)\n",
      "Requirement already satisfied: ipython==7.33.0 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from -r requirements.txt (line 3)) (7.33.0)\n",
      "Requirement already satisfied: pandas==1.3.5 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from -r requirements.txt (line 4)) (1.3.5)\n",
      "Requirement already satisfied: scikit-learn==1.0.2 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from -r requirements.txt (line 5)) (1.0.2)\n",
      "Requirement already satisfied: seaborn==0.12.0 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from -r requirements.txt (line 6)) (0.12.0)\n",
      "Requirement already satisfied: transformers==4.22.1 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from -r requirements.txt (line 7)) (4.22.1)\n",
      "Requirement already satisfied: torch==1.13.0 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from -r requirements.txt (line 8)) (1.13.0)\n",
      "Requirement already satisfied: tensorflow==2.8.0 in c:\\users\\shahr\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 9)) (2.8.0)\n",
      "Collecting protobuf==3.20.0\n",
      "  Using cached protobuf-3.20.0-cp37-cp37m-win_amd64.whl (905 kB)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from scipy==1.7.3->-r requirements.txt (line 1)) (1.21.6)\n",
      "Requirement already satisfied: debugpy>=1.0 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from ipykernel==6.15.3->-r requirements.txt (line 2)) (1.5.1)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from ipykernel==6.15.3->-r requirements.txt (line 2)) (1.5.6)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from ipykernel==6.15.3->-r requirements.txt (line 2)) (0.1.6)\n",
      "Requirement already satisfied: psutil in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from ipykernel==6.15.3->-r requirements.txt (line 2)) (5.9.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from ipykernel==6.15.3->-r requirements.txt (line 2)) (7.4.9)\n",
      "Requirement already satisfied: packaging in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from ipykernel==6.15.3->-r requirements.txt (line 2)) (23.0)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from ipykernel==6.15.3->-r requirements.txt (line 2)) (6.2)\n",
      "Requirement already satisfied: traitlets>=5.1.0 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from ipykernel==6.15.3->-r requirements.txt (line 2)) (5.8.1)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from ipykernel==6.15.3->-r requirements.txt (line 2)) (23.2.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from ipython==7.33.0->-r requirements.txt (line 3)) (3.0.36)\n",
      "Requirement already satisfied: decorator in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from ipython==7.33.0->-r requirements.txt (line 3)) (5.1.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from ipython==7.33.0->-r requirements.txt (line 3)) (65.6.3)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from ipython==7.33.0->-r requirements.txt (line 3)) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from ipython==7.33.0->-r requirements.txt (line 3)) (0.18.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from ipython==7.33.0->-r requirements.txt (line 3)) (0.4.6)\n",
      "Requirement already satisfied: pygments in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from ipython==7.33.0->-r requirements.txt (line 3)) (2.14.0)\n",
      "Requirement already satisfied: backcall in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from ipython==7.33.0->-r requirements.txt (line 3)) (0.2.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from pandas==1.3.5->-r requirements.txt (line 4)) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from pandas==1.3.5->-r requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from scikit-learn==1.0.2->-r requirements.txt (line 5)) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from scikit-learn==1.0.2->-r requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from seaborn==0.12.0->-r requirements.txt (line 6)) (4.1.1)\n",
      "Requirement already satisfied: matplotlib>=3.1 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from seaborn==0.12.0->-r requirements.txt (line 6)) (3.5.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from transformers==4.22.1->-r requirements.txt (line 7)) (0.11.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from transformers==4.22.1->-r requirements.txt (line 7)) (6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from transformers==4.22.1->-r requirements.txt (line 7)) (2.28.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from transformers==4.22.1->-r requirements.txt (line 7)) (2022.10.31)\n",
      "Requirement already satisfied: filelock in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from transformers==4.22.1->-r requirements.txt (line 7)) (3.9.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from transformers==4.22.1->-r requirements.txt (line 7)) (4.64.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from transformers==4.22.1->-r requirements.txt (line 7)) (0.12.1)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from transformers==4.22.1->-r requirements.txt (line 7)) (6.0.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\shahr\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow==2.8.0->-r requirements.txt (line 9)) (1.1.2)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Using cached wrapt-1.14.1-cp37-cp37m-win_amd64.whl (35 kB)\n",
      "Collecting absl-py>=0.4.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "     -------------------------------------- 126.5/126.5 kB 1.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from tensorflow==2.8.0->-r requirements.txt (line 9)) (1.16.0)\n",
      "Collecting flatbuffers>=1.12\n",
      "  Downloading flatbuffers-23.1.4-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\shahr\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow==2.8.0->-r requirements.txt (line 9)) (1.50.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\shahr\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow==2.8.0->-r requirements.txt (line 9)) (1.6.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\shahr\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow==2.8.0->-r requirements.txt (line 9)) (3.7.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\shahr\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow==2.8.0->-r requirements.txt (line 9)) (0.2.0)\n",
      "Collecting libclang>=9.0.1\n",
      "  Downloading libclang-15.0.6.1-py2.py3-none-win_amd64.whl (23.2 MB)\n",
      "     --------------------------------------- 23.2/23.2 MB 34.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in c:\\users\\shahr\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow==2.8.0->-r requirements.txt (line 9)) (2.8.0)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Using cached tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
      "Requirement already satisfied: gast>=0.2.1 in c:\\users\\shahr\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow==2.8.0->-r requirements.txt (line 9)) (0.5.3)\n",
      "Collecting keras<2.9,>=2.8.0rc0\n",
      "  Using cached keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.29.0-cp37-cp37m-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 31.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from astunparse>=1.6.0->tensorflow==2.8.0->-r requirements.txt (line 9)) (0.37.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from jedi>=0.16->ipython==7.33.0->-r requirements.txt (line 3)) (0.8.3)\n",
      "Requirement already satisfied: jupyter-core>=4.9.2 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel==6.15.3->-r requirements.txt (line 2)) (4.11.1)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel==6.15.3->-r requirements.txt (line 2)) (0.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from matplotlib>=3.1->seaborn==0.12.0->-r requirements.txt (line 6)) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from matplotlib>=3.1->seaborn==0.12.0->-r requirements.txt (line 6)) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from matplotlib>=3.1->seaborn==0.12.0->-r requirements.txt (line 6)) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from matplotlib>=3.1->seaborn==0.12.0->-r requirements.txt (line 6)) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from matplotlib>=3.1->seaborn==0.12.0->-r requirements.txt (line 6)) (9.4.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython==7.33.0->-r requirements.txt (line 3)) (0.2.6)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\shahr\\appdata\\roaming\\python\\python37\\site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0->-r requirements.txt (line 9)) (0.4.6)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\shahr\\appdata\\roaming\\python\\python37\\site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0->-r requirements.txt (line 9)) (2.2.2)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\shahr\\appdata\\roaming\\python\\python37\\site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0->-r requirements.txt (line 9)) (3.4.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\shahr\\appdata\\roaming\\python\\python37\\site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0->-r requirements.txt (line 9)) (2.14.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from requests->transformers==4.22.1->-r requirements.txt (line 7)) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from requests->transformers==4.22.1->-r requirements.txt (line 7)) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from requests->transformers==4.22.1->-r requirements.txt (line 7)) (3.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from requests->transformers==4.22.1->-r requirements.txt (line 7)) (1.26.14)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from importlib-metadata->transformers==4.22.1->-r requirements.txt (line 7)) (3.11.0)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\shahr\\appdata\\roaming\\python\\python37\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0->-r requirements.txt (line 9)) (5.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\shahr\\appdata\\roaming\\python\\python37\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0->-r requirements.txt (line 9)) (1.3.1)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from jupyter-core>=4.9.2->jupyter-client>=6.1.12->ipykernel==6.15.3->-r requirements.txt (line 2)) (227)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8.0->-r requirements.txt (line 9)) (2.1.2)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Installing collected packages: tf-estimator-nightly, tensorboard-plugin-wit, pyasn1, libclang, keras, flatbuffers, wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, rsa, pyasn1-modules, protobuf, opt-einsum, oauthlib, absl-py\n",
      "Successfully installed absl-py-1.4.0 flatbuffers-23.1.4 keras-2.8.0 libclang-15.0.6.1 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-3.20.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 rsa-4.9 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-io-gcs-filesystem-0.29.0 termcolor-2.2.0 tf-estimator-nightly-2.8.0.dev2021122109 wrapt-1.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.4.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
      "     --------------------------------------- 12.8/12.8 MB 46.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from en-core-web-sm==3.4.1) (3.4.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.28.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.7)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (65.6.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (23.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\n",
      "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.1.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (6.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.11)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.1.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.4)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.2)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\shahr\\miniconda3\\envs\\nlp-task\\lib\\site-packages (from click<9.0.0,>=7.1.1->typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (6.0.0)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.4.1\n",
      "✔ Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-19 23:25:00.359458: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2023-01-19 23:25:00.359749: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-19 23:25:03.150376: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\n",
      "2023-01-19 23:25:03.150519: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-19 23:25:03.153362: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: DESKTOP-O07VI0E\n",
      "2023-01-19 23:25:03.153856: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: DESKTOP-O07VI0E\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import re\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('carbonated_soft_drinks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Creating corpus from dataset by removing tag, puncs, special chars.\n",
    "'''\n",
    "\n",
    "\n",
    "def corpus_maker():\n",
    "        \n",
    "        stop_words = ['bvc', 'ginger' ,'ale', 'cherry', 'soft', 'bottle', 'oz', 'can', 'diet', 'zero', 'beverage', 'soda', \n",
    "        'drink', 'bott', 'bottled', 'pepper', 'classic', 'sugar', 'dr', 'max']  \n",
    "        raw_corpus = []\n",
    "        for i in range(len(data)):\n",
    "\n",
    "        #Remove punctuations\n",
    "                text = re.sub('[^a-zA-Z]', ' ', data['item_name'][i])\n",
    "\n",
    "        #Convert to lowercase\n",
    "                text = text.lower()\n",
    "        \n",
    "        #remove tags\n",
    "                text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
    "\n",
    "        # remove special characters and digits\n",
    "                text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "\n",
    "        ##Convert to list from string\n",
    "                text = text.split()\n",
    "\n",
    "                text = [word for word in text if not word in stop_words] \n",
    "                text = ' '.join(text)\n",
    "                raw_corpus.append(text)\n",
    "        return raw_corpus\n",
    "raw_corpus = corpus_maker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Find the empty corpus indexes and remove similar rows from dataset  \n",
    "'''\n",
    "\n",
    "def polish_data():\n",
    "\n",
    "    empt_ind  = []\n",
    "    j = 0\n",
    "    while j < len(raw_corpus):\n",
    "        if raw_corpus[j] == '':\n",
    "            empt_ind.append(j)\n",
    "        j += 1\n",
    "            \n",
    "    polish_data = data.drop(empt_ind)\n",
    "    polish_data = polish_data.reset_index(drop=True)\n",
    "    corpus = [k for k in raw_corpus if not k == '']\n",
    "    return polish_data, corpus\n",
    "p_data, corpus = polish_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 936x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' \n",
    "find the most frequent one word in dataset \n",
    "'''\n",
    "\n",
    "def get_top_one_words():\n",
    "        \n",
    "        vec = CountVectorizer()\n",
    "        bag_of_words = vec.fit_transform(corpus)\n",
    "        sum_words = bag_of_words.sum(axis=0) \n",
    "        words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "        #Convert most freq words to dataframe for plotting bar plot\n",
    "        top_words = words_freq[:4]\n",
    "        top_words_dict = {i[0]:i[1] for i in top_words}\n",
    "        top_df = pd.DataFrame(top_words)\n",
    "        top_df.columns=['Word', 'Freq']\n",
    "        #Barplot of most freq words\n",
    "        \n",
    "        sns.set(rc={'figure.figsize':(13,8)})\n",
    "        g = sns.barplot(x='Word', y='Freq', data=top_df)\n",
    "        g.set_xticklabels(g.get_xticklabels(), rotation=30)\n",
    "        return top_words_dict\n",
    "top_words_dict = get_top_one_words()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 936x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' \n",
    "Find the most frequent two consecutive words in dataset \n",
    "'''\n",
    "\n",
    "def get_top_two_words():\n",
    "       \n",
    "        vec = CountVectorizer(ngram_range=(2,2), max_features=2000)\n",
    "        bag_of_words = vec.fit_transform(corpus)\n",
    "        sum_words = bag_of_words.sum(axis=0) \n",
    "        words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "        #Convert most freq words to dataframe for plotting bar plot\n",
    "        top2_words = words_freq[:4]\n",
    "        top2_words_dict = {i[0]:i[1] for i in top2_words}\n",
    "        top2_df = pd.DataFrame(top2_words)\n",
    "        top2_df.columns=['bi_gram', 'Freq']\n",
    "        #Barplot of most freq words\n",
    "        \n",
    "        sns.set(rc={'figure.figsize':(13,8)})\n",
    "        g = sns.barplot(x='bi_gram', y='Freq', data=top2_df)\n",
    "        g.set_xticklabels(g.get_xticklabels(), rotation=30)\n",
    "        return top2_words_dict\n",
    "top2_words_dict = get_top_two_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Merge one most frequent and two most frequent word in case of overlap.\n",
    "Assign each most frequent item to its related brand. \n",
    "This function returns a dictionary in which keys are the most frequent words and values are the brands.\n",
    "'''\n",
    "\n",
    "def brand_extracted():\n",
    "    \n",
    "    list_sep_2nd = [j for i in top2_words_dict.keys() for j in i.split()]\n",
    "    merge_top = []\n",
    "    for i in top_words_dict.keys():\n",
    "        if i not in list_sep_2nd:\n",
    "                merge_top.append(i)\n",
    "    merge_top = merge_top + list(top2_words_dict.keys())\n",
    "    brands = {}\n",
    "    for item in merge_top:\n",
    "        if 'coke' in item or 'coca' in item:\n",
    "            brands[item] = 'coke'\n",
    "        elif 'pepsi' in item or 'dew' in item:\n",
    "            brands[item] = 'pepsi'\n",
    "        else:\n",
    "            brands[item] = item\n",
    "    return brands\n",
    "brands = brand_extracted()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "This function uses NER(NAMED ENTITY RECOGNITION) and matching dictionary to extract the labels for each dataset record.  \n",
    "'''\n",
    "\n",
    "def ner_label_creater():\n",
    "    \n",
    "    nlp = en_core_web_sm.load()\n",
    "    labels = {}\n",
    "    for i,j in enumerate(corpus):\n",
    "        for k in brands.keys():\n",
    "            if k in j:\n",
    "                labels[i] = brands[k]\n",
    "                break\n",
    "        if i not in labels.keys():\n",
    "            nlp_created = nlp(j)\n",
    "            for it in nlp_created.ents:\n",
    "                if it.label_ == 'ORG':\n",
    "                    tmp = str(it).split(' ')\n",
    "                    if len(tmp) > 1:\n",
    "                        for k in tmp:\n",
    "                            if k in brands.keys():\n",
    "                                labels[i] = brands[k]\n",
    "                    else:\n",
    "                        if str(it) in brands.keys():\n",
    "                            labels[i] = brands[str(it)]\n",
    "    return labels\n",
    "labels = ner_label_creater()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "This function added two columns to dataset namely brands and label and then save this csv file to directory. \n",
    "'''\n",
    "\n",
    "def create_branded_data():\n",
    "\n",
    "    possible_labels = list(set(labels.values()))\n",
    "    label_dict = {}\n",
    "    for index, possible_label in enumerate(possible_labels):\n",
    "        label_dict[possible_label] = index\n",
    "\n",
    "    selected_data = p_data.loc[p_data.index.isin(labels.keys())]\n",
    "    selected_data = selected_data.reset_index(drop=True)\n",
    "    selected_data['brands'] = labels.values()\n",
    "    selected_data['label'] = selected_data.brands.replace(label_dict)\n",
    "    if not os.path.exists('branded_data.csv'):\n",
    "        selected_data.to_csv('branded_data.csv', index=False)\n",
    "    return selected_data, label_dict\n",
    "selected_data, label_dict = create_branded_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\shahr\\miniconda3\\envs\\vahe-project\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "Create train, validation and test dataset and use bert tokenizer to tokenize each datasets \n",
    "'''\n",
    "\n",
    "def data_prep_train():\n",
    "   \n",
    "    X = selected_data['item_name'].values\n",
    "    y = selected_data['brands'].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.20, random_state = 0)\n",
    "    selected_data['data_type'] = selected_data.apply(lambda x: 'train' if x['item_name'] in X_train else \n",
    "    ('val' if x['item_name'] in X_val else 'test'), axis=1)\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
    "                                          do_lower_case=True)\n",
    "    encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    selected_data[selected_data['data_type'] == 'train'].item_name.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=256, \n",
    "    return_tensors='pt')\n",
    "\n",
    "    encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    selected_data[selected_data['data_type'] == 'val'].item_name.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=256, \n",
    "    return_tensors='pt')\n",
    "\n",
    "    encoded_data_test = tokenizer.batch_encode_plus(\n",
    "    selected_data[selected_data['data_type'] == 'test'].item_name.values, \n",
    "    add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "    return_attention_mask=True, # Construct attn. masks\n",
    "    pad_to_max_length=True, # Pad or truncate the sentence to `max_length`\n",
    "    max_length=256, # Pad & truncate \n",
    "    return_tensors='pt')  # Return pytorch tensors\n",
    "\n",
    "    input_ids_train = encoded_data_train['input_ids'] \n",
    "    attention_masks_train = encoded_data_train['attention_mask']\n",
    "    labels_train = torch.tensor(selected_data[selected_data['data_type'] == 'train'].label.values)\n",
    "\n",
    "    input_ids_val = encoded_data_val['input_ids']\n",
    "    attention_masks_val = encoded_data_val['attention_mask']\n",
    "    labels_val = torch.tensor(selected_data[selected_data['data_type'] == 'val'].label.values)\n",
    "\n",
    "    input_ids_test = encoded_data_test['input_ids']\n",
    "    attention_masks_test = encoded_data_test['attention_mask']\n",
    "    labels_test = torch.tensor(selected_data[selected_data['data_type'] == 'test'].label.values)\n",
    "\n",
    "    # Convert the datasets tensors\n",
    "    dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "    dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n",
    "    dataset_test = TensorDataset(input_ids_test, attention_masks_test, labels_test)\n",
    "\n",
    "    return dataset_train, dataset_val, dataset_test\n",
    "dataset_train, dataset_val, dataset_test = data_prep_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\shahr\\miniconda3\\envs\\vahe-project\\lib\\site-packages\\transformers\\optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "0/478 loss: 1.0998873710632324 \n",
      "Epoch:  1\n",
      "20/478 loss: 1.143504023551941 \n",
      "Epoch:  1\n",
      "40/478 loss: 1.1449387868245442 \n",
      "Epoch:  1\n",
      "60/478 loss: 1.130900800228119 \n",
      "Epoch:  1\n",
      "80/478 loss: 1.1138249397277833 \n",
      "Epoch:  1\n",
      "100/478 loss: 1.0914943516254425 \n",
      "Epoch:  1\n",
      "120/478 loss: 1.0915969184466772 \n",
      "Epoch:  1\n",
      "140/478 loss: 1.0765775367617607 \n",
      "Epoch:  1\n",
      "160/478 loss: 1.0660556488566928 \n",
      "Epoch:  1\n",
      "180/478 loss: 1.0571697890758514 \n",
      "Epoch:  1\n",
      "200/478 loss: 1.0600426251238042 \n",
      "Epoch:  1\n",
      "220/478 loss: 1.043540398279826 \n",
      "Epoch:  1\n",
      "240/478 loss: 1.0394767476962163 \n",
      "Epoch:  1\n",
      "260/478 loss: 1.0166858179228646 \n",
      "Epoch:  1\n",
      "280/478 loss: 1.0343721071879068 \n",
      "Epoch:  1\n",
      "300/478 loss: 1.031073458492756 \n",
      "Epoch:  1\n",
      "320/478 loss: 1.024872148738188 \n",
      "Epoch:  1\n",
      "340/478 loss: 1.0112052261829376 \n",
      "Epoch:  1\n",
      "360/478 loss: 1.0085526704788208 \n",
      "Epoch:  1\n",
      "380/478 loss: 1.0068814694881438 \n",
      "Epoch:  1\n",
      "400/478 loss: 1.0068890651067097 \n",
      "Epoch:  1\n",
      "420/478 loss: 0.9925442717292092 \n",
      "Epoch:  1\n",
      "440/478 loss: 0.9878643854804661 \n",
      "Epoch:  1\n",
      "460/478 loss: 0.9784900123874346 \n",
      "Training loss at end of epoch 1 is 0.9784900123874346:\n",
      "0/101 validation loss: 0.81428462266922 \n",
      "5/101 validation loss: 0.7958472669124603 \n",
      "10/101 validation loss: 0.7482304374376932 \n",
      "15/101 validation loss: 0.733146920800209 \n",
      "20/101 validation loss: 0.7715825200080871 \n",
      "25/101 validation loss: 0.7809240420659384 \n",
      "30/101 validation loss: 0.7755114947046552 \n",
      "35/101 validation loss: 0.7669614925980568 \n",
      "40/101 validation loss: 0.7672260469860501 \n",
      "45/101 validation loss: 0.7713153600692749 \n",
      "50/101 validation loss: 0.7691309993917291 \n",
      "55/101 validation loss: 0.7805335174004236 \n",
      "60/101 validation loss: 0.7778132741267865 \n",
      "65/101 validation loss: 0.7791441806725093 \n",
      "70/101 validation loss: 0.7851364215215048 \n",
      "75/101 validation loss: 0.7935532182455063 \n",
      "80/101 validation loss: 0.7987537348971647 \n",
      "85/101 validation loss: 0.8027162982357873 \n",
      "90/101 validation loss: 0.7946516934194063 \n",
      "95/101 validation loss: 0.7972790330648423 \n",
      "100/101 validation loss: 0.7975481010618664 \n",
      "avg loss afte 1 epoch is 0.7975481010618664\n",
      "Epoch:  2\n",
      "0/478 loss: 0.03143112659454346 \n",
      "Epoch:  2\n",
      "20/478 loss: 0.062186092138290405 \n",
      "Epoch:  2\n",
      "40/478 loss: 0.09044781879142479 \n",
      "Epoch:  2\n",
      "60/478 loss: 0.11219536406653267 \n",
      "Epoch:  2\n",
      "80/478 loss: 0.127725664911599 \n",
      "Epoch:  2\n",
      "100/478 loss: 0.14651618202527364 \n",
      "Epoch:  2\n",
      "120/478 loss: 0.16567497099599532 \n",
      "Epoch:  2\n",
      "140/478 loss: 0.18379882350564003 \n",
      "Epoch:  2\n",
      "160/478 loss: 0.19775051601005322 \n",
      "Epoch:  2\n",
      "180/478 loss: 0.22159932816729827 \n",
      "Epoch:  2\n",
      "200/478 loss: 0.2334295664514814 \n",
      "Epoch:  2\n",
      "220/478 loss: 0.24416689409150016 \n",
      "Epoch:  2\n",
      "240/478 loss: 0.2570616167944831 \n",
      "Epoch:  2\n",
      "260/478 loss: 0.2650125136500911 \n",
      "Epoch:  2\n",
      "280/478 loss: 0.27814440696667403 \n",
      "Epoch:  2\n",
      "300/478 loss: 0.2866630807518959 \n",
      "Epoch:  2\n",
      "320/478 loss: 0.2923216834300902 \n",
      "Epoch:  2\n",
      "340/478 loss: 0.3010536999929519 \n",
      "Epoch:  2\n",
      "360/478 loss: 0.30462272361267445 \n",
      "Epoch:  2\n",
      "380/478 loss: 0.3111134483055635 \n",
      "Epoch:  2\n",
      "400/478 loss: 0.3153970162073771 \n",
      "Epoch:  2\n",
      "420/478 loss: 0.3214601226474928 \n",
      "Epoch:  2\n",
      "440/478 loss: 0.32685082897226864 \n",
      "Epoch:  2\n",
      "460/478 loss: 0.33160222818454105 \n",
      "Training loss at end of epoch 2 is 0.33160222818454105:\n",
      "0/101 validation loss: 0.020653507926247337 \n",
      "5/101 validation loss: 0.037098792584046074 \n",
      "10/101 validation loss: 0.047905596594015755 \n",
      "15/101 validation loss: 0.060925675630569456 \n",
      "20/101 validation loss: 0.07887059335525219 \n",
      "25/101 validation loss: 0.09261423239001522 \n",
      "30/101 validation loss: 0.10354214587381907 \n",
      "35/101 validation loss: 0.11375438961489447 \n",
      "40/101 validation loss: 0.1222208559513092 \n",
      "45/101 validation loss: 0.13607097633423343 \n",
      "50/101 validation loss: 0.14441456831991673 \n",
      "55/101 validation loss: 0.15434322573921896 \n",
      "60/101 validation loss: 0.16189714126727162 \n",
      "65/101 validation loss: 0.17074993763651167 \n",
      "70/101 validation loss: 0.17903621825906965 \n",
      "75/101 validation loss: 0.18673504848737973 \n",
      "80/101 validation loss: 0.19532020939023872 \n",
      "85/101 validation loss: 0.20283984755858397 \n",
      "90/101 validation loss: 0.20650411397218704 \n",
      "95/101 validation loss: 0.21297356922452043 \n",
      "100/101 validation loss: 0.2179103564648401 \n",
      "avg loss afte 2 epoch is 0.2179103564648401\n",
      "Epoch:  3\n",
      "0/478 loss: 0.008592000421212644 \n",
      "Epoch:  3\n",
      "20/478 loss: 0.019910010695457458 \n",
      "Epoch:  3\n",
      "40/478 loss: 0.02667560939695321 \n",
      "Epoch:  3\n",
      "60/478 loss: 0.03290652139828755 \n",
      "Epoch:  3\n",
      "80/478 loss: 0.03796050233661004 \n",
      "Epoch:  3\n",
      "100/478 loss: 0.043155134828002366 \n",
      "Epoch:  3\n",
      "120/478 loss: 0.050121842189268635 \n",
      "Epoch:  3\n",
      "140/478 loss: 0.0556795671582222 \n",
      "Epoch:  3\n",
      "160/478 loss: 0.0602263797793472 \n",
      "Epoch:  3\n",
      "180/478 loss: 0.06310135280263834 \n",
      "Epoch:  3\n",
      "200/478 loss: 0.07236242041749469 \n",
      "Epoch:  3\n",
      "220/478 loss: 0.07705818365017574 \n",
      "Epoch:  3\n",
      "240/478 loss: 0.0831599597071038 \n",
      "Epoch:  3\n",
      "260/478 loss: 0.08770116634907262 \n",
      "Epoch:  3\n",
      "280/478 loss: 0.09046737021870083 \n",
      "Epoch:  3\n",
      "300/478 loss: 0.09346875222399831 \n",
      "Epoch:  3\n",
      "320/478 loss: 0.09572130900162917 \n",
      "Epoch:  3\n",
      "340/478 loss: 0.09911655928149368 \n",
      "Epoch:  3\n",
      "360/478 loss: 0.10328150596191633 \n",
      "Epoch:  3\n",
      "380/478 loss: 0.1068895656396361 \n",
      "Epoch:  3\n",
      "400/478 loss: 0.10815730993298517 \n",
      "Epoch:  3\n",
      "420/478 loss: 0.110640264408929 \n",
      "Epoch:  3\n",
      "440/478 loss: 0.11366066294656672 \n",
      "Epoch:  3\n",
      "460/478 loss: 0.1140512770248784 \n",
      "Training loss at end of epoch 3 is 0.1140512770248784:\n",
      "0/101 validation loss: 0.004292549088943836 \n",
      "5/101 validation loss: 0.007286549630490216 \n",
      "10/101 validation loss: 0.009686264892419178 \n",
      "15/101 validation loss: 0.01303059947879418 \n",
      "20/101 validation loss: 0.01817862388301403 \n",
      "25/101 validation loss: 0.021790161884079378 \n",
      "30/101 validation loss: 0.024103898935172022 \n",
      "35/101 validation loss: 0.02710177645087242 \n",
      "40/101 validation loss: 0.029298765256124383 \n",
      "45/101 validation loss: 0.03652192064775871 \n",
      "50/101 validation loss: 0.03909289794710447 \n",
      "55/101 validation loss: 0.04202201948673637 \n",
      "60/101 validation loss: 0.04451123679226095 \n",
      "65/101 validation loss: 0.04787503435675587 \n",
      "70/101 validation loss: 0.05058746628071133 \n",
      "75/101 validation loss: 0.05326562002301216 \n",
      "80/101 validation loss: 0.05673433726621886 \n",
      "85/101 validation loss: 0.059287386015057564 \n",
      "90/101 validation loss: 0.06041132756432549 \n",
      "95/101 validation loss: 0.06280433038069357 \n",
      "100/101 validation loss: 0.06415245067032557 \n",
      "avg loss afte 3 epoch is 0.06415245067032557\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "This function use train and validation dataset to retrain BERT with our custom dataset \n",
    "'''\n",
    "\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "EPOCHS = 3\n",
    "def train_model():\n",
    "\n",
    "    dataloader_train = DataLoader(dataset_train, \n",
    "                              sampler=RandomSampler(dataset_train),  # Create the DataLoader\n",
    "                              batch_size=BATCH_SIZE)\n",
    "\n",
    "    dataloader_validation = DataLoader(dataset_val, \n",
    "                                   sampler=SequentialSampler(dataset_val), \n",
    "                                   batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "    # linear classification layer on top.\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",  # Use the 12-layer BERT model, with an uncased vocab\n",
    "                                                      num_labels=len(label_dict), # The number of output labels--3 for multiclass classification\n",
    "                                                      output_attentions=False,  # Whether the model returns attentions weights\n",
    "                                                      output_hidden_states=False)  # Whether the model returns all hidden-states\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    writer = SummaryWriter()\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-5, eps=1e-8)\n",
    "    step = 0\n",
    "    val_step = 0\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for epoch_num in range(EPOCHS):\n",
    "\n",
    "        model.train()   # Put the model into training mode. Don't be mislead--the call to \n",
    "                        # train just changes the *mode*, it doesn't *perform* the training.\n",
    "                        # dropout and batchnorm layers behave differently during training\n",
    "\n",
    "        train_loss = 0   # Reset the total loss for this epoch\n",
    "        \n",
    "\n",
    "        for step_num, batch_data in enumerate(dataloader_train):\n",
    "        \n",
    "\n",
    "            if step_num % 20 == 0:  # Progress update every 20 batches due to lack of resources.\n",
    "\n",
    "                step += 1\n",
    "\n",
    "                batch = tuple(b.to(device) for b in batch_data)     # Unpack this training batch from our dataloader. \n",
    "                                                                    \n",
    "                                                                    # As we unpack the batch, we'll also copy each tensor to the device using \n",
    "                                                                    # `to` method.\n",
    "                                                                    #\n",
    "                                                                    # `batch` contains three pytorch tensors:\n",
    "                                                                    #   [0]: input ids \n",
    "                                                                    #   [1]: attention masks\n",
    "                                                                    #   [2]: labels\n",
    "                \n",
    "                inputs = {'input_ids':      batch[0],\n",
    "                        'attention_mask': batch[1],\n",
    "                        'labels':         batch[2],\n",
    "                        }       \n",
    "\n",
    "                model.zero_grad()       # Always clear any previously calculated gradients before performing a\n",
    "                                        # backward pass. PyTorch doesn't do this automatically because \n",
    "                                        # accumulating the gradients is \"convenient while training RNNs\"\n",
    "\n",
    "                outputs = model(**inputs)       # Perform a forward pass (evaluate the model on this training batch).\n",
    "                                                 \n",
    "                                                # It returns different numbers of parameters depending on what arguments\n",
    "                                                # arge given and what flags are set. For our useage here, it returns\n",
    "                                                # the loss (because we provided labels) and the \"logits\"--the model\n",
    "                                                # outputs prior to activation.\n",
    "                batch_loss = outputs[0]\n",
    "                train_loss += batch_loss.item()     # Accumulate the training loss over all of the batches so that we can\n",
    "                                                    # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "                                                    # single value; the `.item()` function just returns the Python value \n",
    "                                                    # from the tensor.\n",
    "\n",
    "\n",
    "\n",
    "                writer.add_scalar(\"Training_Loss\", train_loss / step, step - 1)\n",
    "                \n",
    "                # Perform a backward pass to calculate the gradients\n",
    "                batch_loss.backward()\n",
    "\n",
    "                # Clip the norm of the gradients to 1.0.\n",
    "                # This is to help prevent the \"exploding gradients\" problem\n",
    "                torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=1.0)\n",
    "\n",
    "                # Update parameters and take a step using the computed gradient.\n",
    "                # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "                # modified based on their gradients, the learning rate, etc.\n",
    "                optimizer.step()\n",
    "                \n",
    "                print('Epoch: ', epoch_num + 1)\n",
    "                print(\"\\r\" + \"{0}/{1} loss: {2} \".format(step_num, len(dataloader_train), train_loss / step))\n",
    "\n",
    "        if not os.path.exists('trained_bert_models'):\n",
    "            os.makedirs('trained_bert_models')\n",
    "        torch.save(model, f'trained_bert_models/finetuned_BERT_epoch_{epoch_num + 1}.pt')\n",
    "        \n",
    "        # Calculate the average loss over all of the batches.\n",
    "        loss_train_avg = train_loss/step         \n",
    "        print('Training loss at end of epoch {} is {}:'.format(epoch_num + 1, loss_train_avg))\n",
    "\n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "\n",
    "        model.eval()    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "                        # during evaluation.\n",
    "\n",
    "\n",
    "        # Tracking variables\n",
    "        loss_val_total = 0\n",
    "        predictions, true_vals = [], []\n",
    "        \n",
    "        # Evaluate data for one epoch\n",
    "        for num_val_step, batch in enumerate(dataloader_validation):\n",
    "\n",
    "            if num_val_step % 5 == 0:\n",
    "\n",
    "                val_step += 1\n",
    "            \n",
    "                batch = tuple(b.to(device) for b in batch)\n",
    "                \n",
    "                inputs = {'input_ids':      batch[0],\n",
    "                        'attention_mask': batch[1],\n",
    "                        'labels':         batch[2],\n",
    "                        }\n",
    "\n",
    "\n",
    "                # Tell pytorch not to bother with constructing the compute graph during\n",
    "                # the forward pass, since this is only needed for backprop (training).\n",
    "                with torch.no_grad():        \n",
    "                    outputs = model(**inputs)\n",
    "                    \n",
    "                loss = outputs[0]\n",
    "                logits = outputs[1]\n",
    "\n",
    "                # Accumulate the validation loss\n",
    "                loss_val_total += loss.item()\n",
    "\n",
    "                writer.add_scalar(\"Validation_Loss\", loss_val_total / val_step, val_step - 1)\n",
    "\n",
    "                print(\"\\r\" + \"{0}/{1} validation loss: {2} \".format(num_val_step, len(dataloader_validation), loss_val_total / val_step))\n",
    "                \n",
    "                # Move logits and labels to CPU\n",
    "                logits = logits.detach().cpu().numpy()\n",
    "                label_ids = inputs['labels'].cpu().numpy()\n",
    "                predictions.append(logits)\n",
    "                true_vals.append(label_ids)\n",
    "        \n",
    "        # Calculate the average loss over all of the batches\n",
    "        loss_val_avg = loss_val_total/val_step \n",
    "        print('avg loss afte {} epoch is {}'.format(epoch_num + 1, loss_val_avg))\n",
    "        predictions = np.concatenate(predictions, axis=0)\n",
    "        true_vals = np.concatenate(true_vals, axis=0)\n",
    "        ######### END OF EVALUATION\n",
    "    \n",
    "    writer.close()\n",
    "\n",
    "    return model\n",
    "\n",
    "model = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m tensorboard.main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 6780), started 0:00:41 ago. (Use '!kill 6780' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b341df4834181f7\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b341df4834181f7\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir runs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "'''  \n",
    "Only uncomment this block if you trained already and want to load from saved model.\n",
    "'''\n",
    "\n",
    "\n",
    "# BATCH_SIZE = 10\n",
    "# EPOCHS = 3\n",
    "\n",
    "# model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",  # Use the 12-layer BERT model, with an uncased vocab\n",
    "#                                                       num_labels=3, # The number of output labels--3 for multiclass classification\n",
    "#                                                       output_attentions=False,  # Whether the model returns attentions weights\n",
    "#                                                       output_hidden_states=False)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "# model = torch.load('trained_bert_models\\\\finetuned_BERT_epoch_3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "This is testing the model with test dataset. The process is similar to the validation part of training function. \n",
    "For more info regarding each step, please check the notes of training function.\n",
    "This function returns the predictions with size of (len(test_dataset), number of classes) and test true labels with size of (len(test_dataset),)\n",
    "The argmax of each prediction return the predicted label.\n",
    "'''\n",
    "\n",
    "def model_eval():\n",
    "\n",
    "    dataloader_test = DataLoader(dataset_test, \n",
    "                                   sampler=SequentialSampler(dataset_test), \n",
    "                                   batch_size=BATCH_SIZE)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    loss_test_total = 0\n",
    "    predictions, true_tests = [], []\n",
    "\n",
    "    for test_step_num, batch in enumerate(dataloader_test):\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                    'attention_mask': batch[1],\n",
    "                    'labels':         batch[2],\n",
    "                    }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        test_loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_test_total += test_loss.item()\n",
    "        \n",
    "        \n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_tests.append(label_ids)\n",
    "\n",
    "        \n",
    "\n",
    "    loss_test_avg = loss_test_total/len(dataloader_test) \n",
    "\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_tests = np.concatenate(true_tests, axis=0)\n",
    "\n",
    "\n",
    "    return predictions, true_tests\n",
    "\n",
    "predictions, true_tests = model_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: coke\n",
      "Accuracy: 513/515\n",
      "\n",
      "Class: pepsi\n",
      "Accuracy: 533/533\n",
      "\n",
      "Class: canada dry\n",
      "Accuracy: 102/115\n",
      "\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       515\n",
      "           1       1.00      1.00      1.00       533\n",
      "           2       1.00      0.89      0.94       115\n",
      "\n",
      "    accuracy                           0.99      1163\n",
      "   macro avg       0.99      0.96      0.97      1163\n",
      "weighted avg       0.99      0.99      0.99      1163\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "Predictions with size of (len(test_dataset), number of classes) and test true labels with size of (len(test_dataset),)\n",
    "The argmax of each prediction return the predicted label.\n",
    "accuracy_per_class function claculate the number of prediction over number of ground truth of each class\n",
    "Classification report present (precision, recall, f1-score) for each class which is represented by row index.  \n",
    "'''\n",
    "\n",
    "def accuracy_per_class(preds, labels):\n",
    "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "    \n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true = labels_flat[labels_flat==label]\n",
    "        print(f'Class: {label_dict_inverse[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')\n",
    "\n",
    "accuracy_per_class(predictions, true_tests)\n",
    "print('\\n')\n",
    "print(classification_report(true_tests.flatten(), np.argmax(predictions, axis=1).flatten()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vahe-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd3329c6ab52d81e4655896fbdd366b63dcfa563eec8ee698a9d16a2b78ccfc9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
